Long running of genetic algorithms -
    With specific initialization (2000 generations, population = 457),
       - We stuck into a local minima -> check best_feature_selection.json and
        generation_info.json
    With random Initization (500 generations, population = 2400)
        - The program was started . Results should be found in genetic_feature_selection.json and
          genetic_generation_info.json

Reproducability -
    For reproduciblity I print the random seed on the first line.
    If we re-use this seed. We can reproduce the result exactly.

Observation -
    Random Forest regresser is giving a very high accuracy of R^2 score > 0.75
    Out of bag score is also 0.75 consistently.
    The oob score ~= R^2 score on the validation set if we use ~200 trees in our emsemble.
    When we use exp data --> This is signifantly bad ~ 0.55. Still do not know why
    Why is this model predicting things so accurately - Still researching.

How to know if the protein/ligand features are being taken into account?
    Fit the model using all the feature
    Random shuffle the protein features of the X_validate variable and check the prediction.
    Do the same thing to check the usage of ligand features.

Observation
    Ligand features have a huge impact on the regressor.

For feature importance calculation -
    1. Using entropy reduction calculation (By default given by our regressor model)
    2. Using permutation feature importance calculation.
       The issue with this is correlation between features. (There needs to be a clustering of features that are correlated.)

Good resource
https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e
Computational complexity - https://christophm.github.io/interpretable-ml-book/feature-importance.html
https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html


THINGS TO DO NEXT - 
1. Using the infrastructure to do feature selection for random forest regression.
    Fitting is expensive, checking is not so expensive.
    We can use Shuffling concept as used by permuation importance selection.
2. Report R^2 score for genetic algorithm runs. (Accuracy is more important) *******
    ==> Number of feature selected is secondary.
3. Support vector regressor with different parameters...
4.
5.
6.
7.
8. ***** Report Writing I am lagging behind - I will do it next
9. Found out a python library called DEAP for doing genetic libraries

NEXT STEPS -
Feature selection (just not genetic algorithms)-
    From Alireza .... ANT COLONY, SWARM optimization ...
==> Manual feature selection and compare our results. (Concrete features for interpretability)
* Maybe 2D features extracted from the proteins..
* Fingerprint - Chemoinformatics? Read about this....
